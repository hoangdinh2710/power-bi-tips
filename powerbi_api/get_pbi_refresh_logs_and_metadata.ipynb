{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Get Power BI refresh logs and metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import library\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import msal\n",
        "from datetime import date\n",
        "from notebookutils import mssparkutils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Set default Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set default parameter \n",
        "client_id = \"\"\n",
        "client_secret = \"\"\n",
        "tenant_name = \"\"\n",
        "authority_url = \"https://login.microsoftonline.com/\" + tenant_name \n",
        "scope = ['https://analysis.windows.net/powerbi/api/.default']\n",
        "\n",
        "# When using Synapse Notebook to get values in Azure Key Value\n",
        "client_id = mssparkutils.credentials.getSecret('key-vault-name','secret-name','key-vault-linked-entity-name')\n",
        "client_secret = mssparkutils.credentials.getSecret('key-vault-name','secret-name','key-vault-linked-entity-name')\n",
        "tenant_name = mssparkutils.credentials.getSecret('key-vault-name','secret-name','key-vault-linked-entity-name') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get token using MASL and Notebookutil package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Use MSAL to grab token\n",
        "app = msal.ConfidentialClientApplication(client_id, authority=authority_url, client_credential=client_secret)\n",
        "result = app.acquire_token_for_client(scopes=scope)\n",
        "\n",
        "#Get latest Power BI Dataset Refresh\n",
        "access_token = result['access_token']\n",
        "\n",
        "# Set header\n",
        "header = {\n",
        "    'Content-Type':'application/json', \n",
        "    'Authorization':f'Bearer {access_token}'\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get Token without additional packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import get_token\n",
        "\n",
        "access_token = get_token()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Query data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get all workspaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "get_all_workspaces_url = 'https://api.powerbi.com/v1.0/myorg/groups'\n",
        "api_call = requests.get(url=get_all_workspaces_url, headers=header)\n",
        "df_get_all_workspaces = pd.DataFrame.from_dict(api_call.json()['value'], orient ='columns')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get all datasets from list of workspaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get all datasets\n",
        "\n",
        "# Set workspace list\n",
        "workspace_id_list = df_get_all_workspaces['id']\n",
        "# Define an empty dataframe\n",
        "df_get_all_datasets = pd.DataFrame()\n",
        "# Loop through workspace\n",
        "for workspace_id in workspace_id_list:\n",
        "    workspace_name = df_get_all_workspaces.query('id == \"{0}\"'.format(workspace_id))[\"name\"].iloc[0]\n",
        "    # Define URL endpoint\n",
        "    get_all_datasets_url =  'https://api.powerbi.com/v1.0/myorg/groups/{0}/datasets'.format(workspace_id)\n",
        "    # Send API call to get data\n",
        "    api_call = requests.get(url=get_all_datasets_url, headers=header)\n",
        "    # If api_call success then proceed:\n",
        "    if api_call.status_code == 200:\n",
        "        # Create dataframe to store data\n",
        "        df = pd.DataFrame.from_dict(api_call.json()['value'], orient ='columns')\n",
        "        # Add workspace id column\n",
        "        df['workspaceId'] = workspace_id\n",
        "        # Add workspace name column\n",
        "        df['workspaceName'] = workspace_name\n",
        "        # Append data\n",
        "        df_get_all_datasets = df_get_all_datasets.append(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get all dataflows from list of workspaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set workspace list\n",
        "workspace_id_list = df_get_all_workspaces['id']\n",
        "# Define an empty dataframe\n",
        "df_get_all_dataflows = pd.DataFrame()\n",
        "# Loop through workspace\n",
        "for workspace_id in workspace_id_list:\n",
        "    workspace_name = df_get_all_workspaces.query('id == \"{0}\"'.format(workspace_id))[\"name\"].iloc[0]\n",
        "    # Define URL endpoint\n",
        "    get_all_dataflows_url =  'https://api.powerbi.com/v1.0/myorg/groups/{0}/dataflows'.format(workspace_id)\n",
        "    # Send API call to get data\n",
        "    api_call = requests.get(url=get_all_dataflows_url, headers=header)\n",
        "    # If api_call success then proceed:\n",
        "    if api_call.status_code == 200:\n",
        "        # Create dataframe to store data\n",
        "        df = pd.DataFrame.from_dict(api_call.json()['value'], orient ='columns')\n",
        "        # Add column\n",
        "        df['workspaceId'] = workspace_id\n",
        "        # Add workspace name column\n",
        "        df['workspaceName'] = workspace_name\n",
        "        # Append data\n",
        "        df_get_all_dataflows = df_get_all_dataflows.append(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get all reports from list of workspaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set workspace list\n",
        "workspace_id_list = df_get_all_workspaces['id']\n",
        "# Define an empty dataframe\n",
        "df_get_all_reports = pd.DataFrame()\n",
        "# Loop through workspace\n",
        "for workspace_id in workspace_id_list:\n",
        "    workspace_name = df_get_all_workspaces.query('id == \"{0}\"'.format(workspace_id))[\"name\"].iloc[0]\n",
        "    # Define URL endpoint\n",
        "    get_all_reports_url =  'https://api.powerbi.com/v1.0/myorg/groups/{0}/reports'.format(workspace_id)\n",
        "    # Send API call to get data\n",
        "    api_call = requests.get(url=get_all_reports_url, headers=header)\n",
        "    # If api_call success then proceed:\n",
        "    if api_call.status_code == 200:\n",
        "        # Create dataframe to store data\n",
        "        df = pd.DataFrame.from_dict(api_call.json()['value'], orient ='columns')\n",
        "        # Add workspace name column\n",
        "        df['workspaceName'] = workspace_name\n",
        "        # Append data\n",
        "        df_get_all_reports = df_get_all_reports.append(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get all dataset refresh history from list of workspaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get dataset refresh history\n",
        "df_get_all_datasets_refresh_history = pd.DataFrame()\n",
        "list_of_ds = df_get_all_datasets.query('isRefreshable == True')['id']\n",
        "# Loop through dataset\n",
        "for dataset_id in list_of_ds:\n",
        "    # Get workspace id\n",
        "    workspace_id = df_get_all_datasets.query('id == \"{0}\"'.format(dataset_id))[\"workspaceId\"].iloc[0]\n",
        "    # Get workspace name\n",
        "    workspace_name = df_get_all_datasets.query('id == \"{0}\"'.format(dataset_id))[\"workspaceName\"].iloc[0]\n",
        "    # Get dataset name\n",
        "    dataset_name = df_get_all_datasets.query('id == \"{0}\"'.format(dataset_id))[\"name\"].iloc[0]\n",
        "    # Define URL endpoint\n",
        "    get_all_dataset_refresh_history_url =  'https://api.powerbi.com/v1.0/myorg/groups/{0}/datasets/{1}/refreshes?$top=10'.format(workspace_id,dataset_id)\n",
        "    # Send API to get data\n",
        "    api_call = requests.get(url=get_all_dataset_refresh_history_url, headers=header)\n",
        "    # If api_call success then proceed:\n",
        "    if api_call.status_code == 200:\n",
        "        # Parse data from json output\n",
        "        df = pd.DataFrame.from_dict(api_call.json()['value'], orient ='columns')\n",
        "        # Add column\n",
        "        df['datasetId'] = dataset_id\n",
        "        # Add column\n",
        "        df['datasetName'] = dataset_name\n",
        "        # Add column\n",
        "        df['workspaceId'] = workspace_id\n",
        "        # Add column\n",
        "        df['workspaceName'] = workspace_name\n",
        "        # Append data\n",
        "        df_get_all_datasets_refresh_history = df_get_all_datasets_refresh_history.append(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get all dataflow refresh history from list of workspaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get dataflow refresh history\n",
        "df_get_all_dataflows_refresh_history = pd.DataFrame()\n",
        "list_of_dataflows = df_get_all_dataflows['objectId']\n",
        "\n",
        "# Loop through each workspace\n",
        "for workspace_id in workspace_id_list:\n",
        "    # Loop through dataflow\n",
        "    for dataflow_id in list_of_dataflows:\n",
        "        # Get workspace name\n",
        "        workspace_name = df_get_all_dataflows.query('objectId == \"{0}\"'.format(dataflow_id))[\"workspaceName\"].iloc[0]\n",
        "        # Get dataflow name\n",
        "        dataflow_name = df_get_all_dataflows.query('objectId == \"{0}\"'.format(dataflow_id))[\"name\"].iloc[0]\n",
        "        # Define URL endpoint\n",
        "        get_all_dataflow_refresh_history_url =  'https://api.powerbi.com/v1.0/myorg/groups/{0}/dataflows/{1}/transactions'.format(workspace_id,dataflow_id)\n",
        "        # Send API to get data\n",
        "        api_call = requests.get(url=get_all_dataflow_refresh_history_url, headers=header)\n",
        "        # If api_call success then proceed:\n",
        "        if api_call.status_code == 200:\n",
        "            # Parse data from json output\n",
        "            df = pd.DataFrame.from_dict(api_call.json()['value'], orient ='columns')\n",
        "            # Add column\n",
        "            df['dataflowId'] = dataflow_id\n",
        "            # Add column\n",
        "            df['dataflowName'] = dataflow_name\n",
        "            # Add column\n",
        "            df['workspaceId'] = workspace_id\n",
        "            # Add column\n",
        "            df['workspaceName'] = workspace_name\n",
        "            # Append data\n",
        "            df_get_all_dataflows_refresh_history = df_get_all_dataflows_refresh_history.append(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get all dataset refresh schedule from list of workspaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get dataset refresh schedule\n",
        "df_get_all_datasets_refresh_schedule = pd.DataFrame()\n",
        "list_of_ds = df_get_all_datasets.query('isRefreshable == True')['id']\n",
        "# Loop through dataset\n",
        "for dataset_id in list_of_ds:\n",
        "    # Get workspace id\n",
        "    workspace_id = df_get_all_datasets.query('id == \"{0}\"'.format(dataset_id))[\"workspaceId\"].iloc[0]\n",
        "    # Get workspace name\n",
        "    workspace_name = df_get_all_datasets.query('id == \"{0}\"'.format(dataset_id))[\"workspaceName\"].iloc[0]\n",
        "    # Get dataset name\n",
        "    dataset_name = df_get_all_datasets.query('id == \"{0}\"'.format(dataset_id))[\"name\"].iloc[0]\n",
        "    # Define URL endpoint\n",
        "    get_all_dataset_refresh_schedule_url = 'https://api.powerbi.com/v1.0/myorg/groups/{0}/datasets/{1}/refreshSchedule'.format(workspace_id,dataset_id)\n",
        "    api_call = requests.get(url=get_all_dataset_refresh_schedule_url, headers=header)\n",
        "    # If api_call success then proceed:\n",
        "    if api_call.status_code == 200:\n",
        "    # Create dictionary to store json data\n",
        "        days_json = '|'.join(api_call.json()['days'])\n",
        "        times_json = '|'.join(api_call.json()['times'])\n",
        "        enabled_json = api_call.json()['enabled']\n",
        "        localTimeZoneId_json = api_call.json()['localTimeZoneId']\n",
        "\n",
        "        this_dict = {\n",
        "            \"days\": [days_json],\n",
        "            \"time\": [times_json],\n",
        "            \"enabled\": [enabled_json],\n",
        "            \"localTimeZoneId\": [localTimeZoneId_json]\n",
        "        }\n",
        "        # Convert dict to data frame\n",
        "        df = pd.DataFrame.from_dict(this_dict, orient ='columns')\n",
        "        # Add column\n",
        "        df['datasetId'] = dataset_id\n",
        "        # Add column\n",
        "        df['datasetName'] = dataset_name\n",
        "        # Add column\n",
        "        df['workspaceId'] = workspace_id\n",
        "        # Add column\n",
        "        df['workspaceName'] = workspace_name\n",
        "        # Append data\n",
        "        df_get_all_datasets_refresh_schedule = df_get_all_datasets_refresh_schedule.append(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Export data to CSV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write all workspaces to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_get_all_workspaces.to_csv(r\"workspaces/workspaces.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write all dataflows to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_get_all_dataflows.to_csv(r\"dataflows/dataflows.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write all datasets to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_get_all_datasets.to_csv(r\"powerbi/datasets/datasets.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write all reports info to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_get_all_reports.to_csv(r\"powerbi/reports/reports.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write dataset refresh schedule info to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_get_all_datasets_refresh_schedule.to_csv(r\"powerbi/datasets_refresh_schedule/refresh_schedule.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write dataset refresh history info to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_get_all_datasets_refresh_history.to_csv(r\"powerbi/datasets_refresh_history/refresh_history_snapshot@{0}.csv\".format(date.today()),index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write dataflow refresh history info to CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_get_all_dataflows_refresh_history.to_csv(r\"powerbi/dataflows_refresh_history/refresh_history_snapshot@{0}.csv\".format(date.today()),index=False)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
